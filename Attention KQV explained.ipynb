{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b2ce9c",
   "metadata": {},
   "source": [
    "#### Attention Key Query and Value\n",
    "\n",
    "The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will compute the similarity between your **query** (text in the search bar) against a set of **keys** (candidate video titles)  in their database, then present you the best matched videos (**values**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04edce",
   "metadata": {},
   "source": [
    "##### attention with one query vector\n",
    "\n",
    "suppose we have a query vector **q**, a list of key vectors $[k_1, k_2,...k_n]$, and a list of value vectors $[v_1, k_v,...v_n]$, $q \\in R^{d}, k_i \\in R^{d}, v_i \\in R^{d}$ \n",
    "\n",
    "attention output by definition is a weighted average of value vectors\n",
    "$$\n",
    "\\sum_{j}^{n}\\alpha_{j}v_{j}\n",
    "$$\n",
    "where $\\alpha_{j}$ are similarity scores and  $\\sum \\alpha_{j} = 1$. If we restrict $\\alpha$ to be a one-hot vector (hard attention), this operation becomes the same as retrieving from a set of elements v with index $\\alpha$. With the restriction removed (soft attention), the attention operation can be thought of as doing \"proportional retrieval\" according to the probability vector $\\alpha$.\n",
    "\n",
    "similarity scores $\\alpha_{j}$ are computed through the similarity between the query q and the j-th key $k_j$\n",
    "$$\n",
    "\\alpha_{j} = sim(q, k_j)\n",
    "$$\n",
    "usually, the similarity function is dot product with softmax. we take the dot product between q and each key to get the raw weights and apply softmax to make sure that the weights sum to 1.\n",
    "\n",
    "the whole attention output given one query vector **q** is\n",
    "$$\n",
    "\\sum_{j}^{n}\\alpha_{j}v_{j} = sim(q, k_j) \\cdot v_{j} = softmax(k_jq^T)\\cdot v_{j}\n",
    "$$\n",
    "from the above definition, we know that the number of keys should be equal to the number of values\n",
    "\n",
    "##### attention with many query vectors\n",
    "\n",
    "suppose we have a list query vectors $[q_1, q_2,...q_m]$, we want to compute the attention output **for each** query vector. \n",
    "\n",
    "We need to reorganize the problem in matrix format: $Q \\in R^{m \\times d}, K \\in R^{n \\times d}, V \\in R^{n \\times d}$\n",
    "\n",
    "we firstly compute the similarity weights:\n",
    "$$\n",
    "A = softmax(QK^T) \\in R^{m \\times n}\n",
    "$$\n",
    "there are m lines and n columns in **A**, each line represents the similarity scores for the corresponding query. softmax function is applied to make sure that weights in each line sum to one.\n",
    "\n",
    "since attention output is a weighted average of value vectors, we can implement this by matrix multiplication\n",
    "$$\n",
    "Attention = AV \\in R^{m \\times d}\n",
    "$$\n",
    "for each query, we compute one attention output which is a weighted sum of values. Since we have m queries, we also have m attention output. \n",
    "$$\n",
    "Attention(Q,K,V) = softmax(QK^T)V\n",
    "$$\n",
    "to sum up:\n",
    "\n",
    "* the number of keys should be equal to the number of values\n",
    "\n",
    "* we have m queries, we have m result. The result matrix shape is always consistent with the query matrix shape\n",
    "* $Q \\in R^{m \\times d}, K \\in R^{n \\times d}, V \\in R^{n \\times d}, Result\\  Attention\\in R^{m \\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d39947",
   "metadata": {},
   "source": [
    "#### Attention layer in encoder\n",
    "\n",
    "Input: $x_e$ with shape $(B, dim, T_{in})$ from previous layer\n",
    "\n",
    "both key, value, and query are  $x_e$, so it is also called self attention layer.\n",
    "\n",
    "since the output shape should be same as query, which is $(B, dim, T_{in})$\n",
    "\n",
    "the attention Mask is $(B, 1, T_{in}, T_{in})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef988a8",
   "metadata": {},
   "source": [
    "#### Attention in decoder\n",
    "\n",
    "Input: $x_d$ with shape $(B, dim, T_{out})$ from previous layer\n",
    "both key, value, and query are  $x_d$, so it is also called self attention layer.\n",
    "\n",
    "since the output shape should be same as query, which is $(B, dim, T_{out})$\n",
    "\n",
    "the attention Mask is $(B, 1, T_{out}, T_{out})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d56d49",
   "metadata": {},
   "source": [
    "#### Cross Attention in decoder\n",
    "\n",
    "Input: $x_d$ with shape $(B, dim, T_{out})$ from previous layer, it can also access the feature map of the encoder which is also called **memory**, with shape $(B, dim, T_{in})$\n",
    "\n",
    "Query: $x_d$\n",
    "\n",
    "Keys and Values: **memory**\n",
    "\n",
    "since the output shape should be same as query, which is $(B, dim, T_{out})$\n",
    "\n",
    "the attention Mask $(B, 1, T_{in}, T_{out})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6af1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
